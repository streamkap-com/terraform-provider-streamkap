// Code generated by tfgen. DO NOT EDIT.

package generated

import (
	"github.com/hashicorp/terraform-plugin-framework-validators/int64validator"
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
)

// DestinationDatabricksModel is the Terraform model for the databricks destination.
type DestinationDatabricksModel struct {
	ID                                  types.String `tfsdk:"id"`
	Name                                types.String `tfsdk:"name"`
	Connector                           types.String `tfsdk:"connector"`
	IngestionMode                       types.String `tfsdk:"ingestion_mode"`
	DatabricksToken                     types.String `tfsdk:"databricks_token"`
	ConnectionURL                       types.String `tfsdk:"connection_url"`
	ConnectionTimeout                   types.Int64  `tfsdk:"connection_timeout"`
	DatabricksCatalog                   types.String `tfsdk:"databricks_catalog"`
	PartitionMode                       types.String `tfsdk:"partition_mode"`
	SchemaEvolution                     types.String `tfsdk:"schema_evolution"`
	TableNamePrefix                     types.String `tfsdk:"table_name_prefix"`
	HardDelete                          types.Bool   `tfsdk:"hard_delete"`
	TasksMax                            types.Int64  `tfsdk:"tasks_max"`
	ConsumerWaitTimeForLargerBatchMs    types.Int64  `tfsdk:"consumer_wait_time_for_larger_batch_ms"`
	Topic2tableMap                      types.Bool   `tfsdk:"topic2table_map"`
	TransformsChangeTopicNameMatchRegex types.String `tfsdk:"transforms_change_topic_name_match_regex"`
	TransformsChangeTopicNameMapping    types.String `tfsdk:"transforms_change_topic_name_mapping"`
}

// DestinationDatabricksSchema returns the Terraform schema for the databricks destination.
func DestinationDatabricksSchema() schema.Schema {
	return schema.Schema{
		MarkdownDescription: "Databricks destination connector",
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Computed:            true,
				MarkdownDescription: "Unique identifier for the destination",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"name": schema.StringAttribute{
				Required:            true,
				MarkdownDescription: "Name of the destination",
			},
			"connector": schema.StringAttribute{
				Computed:            true,
				MarkdownDescription: "Connector type",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"ingestion_mode": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Upsert or append modes are available",
				Default:             stringdefault.StaticString("upsert"),
				Validators: []validator.String{
					stringvalidator.OneOf("upsert", "append"),
				},
			},
			"databricks_token": schema.StringAttribute{
				Optional:            true,
				Sensitive:           true,
				MarkdownDescription: "Token",
			},
			"connection_url": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "JDBC URL",
			},
			"connection_timeout": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Connection Timeout",
				Default:             int64default.StaticInt64(0),
			},
			"databricks_catalog": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "The name of the Databricks catalog to use.",
				Default:             stringdefault.StaticString("hive_metastore"),
			},
			"partition_mode": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Partition tables or not",
				Default:             stringdefault.StaticString("by_topic"),
				Validators: []validator.String{
					stringvalidator.OneOf("by_topic", "by_partition", "by_topic_and_partition"),
				},
			},
			"schema_evolution": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE`",
				Default:             stringdefault.StaticString("basic"),
				Validators: []validator.String{
					stringvalidator.OneOf("basic", "none"),
				},
			},
			"table_name_prefix": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Schema for the associated table name",
				Default:             stringdefault.StaticString("streamkap"),
			},
			"hard_delete": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database",
				Default:             booldefault.StaticBool(false),
			},
			"tasks_max": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "The maximum number of active tasks. NOTE: Increasing this value may increase parallelism and throughput but can also lead to higher costs on databricks side.",
				Default:             int64default.StaticInt64(5),
				Validators: []validator.Int64{
					int64validator.Between(1, 100),
				},
			},
			"consumer_wait_time_for_larger_batch_ms": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "The max wait time for larger batch size (in ms). The bigger the batch size, the the more cost effective loading will be on databricks but latency will grow as a trade-off.",
				Default:             int64default.StaticInt64(10000),
				Validators: []validator.Int64{
					int64validator.Between(500, 300000),
				},
			},
			"topic2table_map": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Falls back to Streamkap's default for tables where no match is found",
				Default:             booldefault.StaticBool(false),
			},
			"transforms_change_topic_name_match_regex": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "Regular expression for matching topic name parts to use as the destination table (database) or file (file storage) name",
			},
			"transforms_change_topic_name_mapping": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "Map source tables to specific destination tables. Input should be the format of `source_table_name:destination_table_name` separated by a new line",
			},
		},
	}
}

// DestinationDatabricksFieldMappings maps Terraform attribute names to API field names.
var DestinationDatabricksFieldMappings = map[string]string{
	"ingestion_mode":                           "ingestion.mode",
	"databricks_token":                         "databricks.token",
	"connection_url":                           "connection.url.user.defined",
	"connection_timeout":                       "connection.timeout.user.defined",
	"databricks_catalog":                       "databricks.catalog.user.defined",
	"partition_mode":                           "partition.mode",
	"schema_evolution":                         "schema.evolution",
	"table_name_prefix":                        "table.name.prefix",
	"hard_delete":                              "hard.delete",
	"tasks_max":                                "tasks.max",
	"consumer_wait_time_for_larger_batch_ms":   "consumer.wait.time.for.larger.batch.ms",
	"topic2table_map":                          "topic2table.map.user.defined",
	"transforms_change_topic_name_match_regex": "transforms.changeTopicName.match.regex.user.defined",
	"transforms_change_topic_name_mapping":     "transforms.changeTopicName.mapping",
}
