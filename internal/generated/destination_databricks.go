// Code generated by tfgen. DO NOT EDIT.

package generated

import (
	"github.com/hashicorp/terraform-plugin-framework-timeouts/resource/timeouts"
	"github.com/hashicorp/terraform-plugin-framework-validators/int64validator"
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
)

// DestinationDatabricksModel is the Terraform model for the databricks destination.
type DestinationDatabricksModel struct {
	ID                                  types.String   `tfsdk:"id"`
	Name                                types.String   `tfsdk:"name"`
	Connector                           types.String   `tfsdk:"connector"`
	IngestionMode                       types.String   `tfsdk:"ingestion_mode"`
	DatabricksToken                     types.String   `tfsdk:"databricks_token"`
	ConnectionURL                       types.String   `tfsdk:"connection_url"`
	ConnectionTimeout                   types.Int64    `tfsdk:"connection_timeout"`
	DatabricksCatalog                   types.String   `tfsdk:"databricks_catalog"`
	PartitionMode                       types.String   `tfsdk:"partition_mode"`
	SchemaEvolution                     types.String   `tfsdk:"schema_evolution"`
	TableNamePrefix                     types.String   `tfsdk:"table_name_prefix"`
	HardDelete                          types.Bool     `tfsdk:"hard_delete"`
	TasksMax                            types.Int64    `tfsdk:"tasks_max"`
	ConsumerWaitTimeForLargerBatchMs    types.Int64    `tfsdk:"consumer_wait_time_for_larger_batch_ms"`
	Topic2tableMap                      types.Bool     `tfsdk:"topic2table_map"`
	TransformsChangeTopicNameMatchRegex types.String   `tfsdk:"transforms_change_topic_name_match_regex"`
	TransformsChangeTopicNameMapping    types.String   `tfsdk:"transforms_change_topic_name_mapping"`
	Timeouts                            timeouts.Value `tfsdk:"timeouts"`
}

// DestinationDatabricksSchema returns the Terraform schema for the databricks destination.
func DestinationDatabricksSchema() schema.Schema {
	return schema.Schema{
		Description: "Manages a Databricks destination connector.",
		MarkdownDescription: "Manages a **Databricks destination connector**.\n\n" +
			"This resource creates and manages a Databricks destination for Streamkap data pipelines.\n\n" +
			"[Documentation](https://docs.streamkap.com/streamkap-provider-for-terraform)",
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Computed:            true,
				Description:         "Unique identifier for the destination",
				MarkdownDescription: "Unique identifier for the destination",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"name": schema.StringAttribute{
				Required:            true,
				Description:         "Name of the destination",
				MarkdownDescription: "Name of the destination",
			},
			"connector": schema.StringAttribute{
				Computed:            true,
				Description:         "Connector type",
				MarkdownDescription: "Connector type",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"ingestion_mode": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Upsert or append modes are available Defaults to \"upsert\". Valid values: upsert, append.",
				MarkdownDescription: "Upsert or append modes are available Defaults to `upsert`. Valid values: `upsert`, `append`.",
				Default:             stringdefault.StaticString("upsert"),
				Validators: []validator.String{
					stringvalidator.OneOf("upsert", "append"),
				},
			},
			"databricks_token": schema.StringAttribute{
				Optional:            true,
				Sensitive:           true,
				Description:         "Token This value is sensitive and will not appear in logs or CLI output.",
				MarkdownDescription: "Token\n\n**Security:** This value is marked sensitive and will not appear in CLI output or logs.",
			},
			"connection_url": schema.StringAttribute{
				Optional:            true,
				Description:         "JDBC URL",
				MarkdownDescription: "JDBC URL",
			},
			"connection_timeout": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "Connection Timeout Defaults to 0.",
				MarkdownDescription: "Connection Timeout Defaults to `0`.",
				Default:             int64default.StaticInt64(0),
			},
			"databricks_catalog": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "The name of the Databricks catalog to use. Defaults to \"hive_metastore\".",
				MarkdownDescription: "The name of the Databricks catalog to use. Defaults to `hive_metastore`.",
				Default:             stringdefault.StaticString("hive_metastore"),
			},
			"partition_mode": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Partition tables or not Defaults to \"by_topic\". Valid values: by_topic, by_partition, by_topic_and_partition.",
				MarkdownDescription: "Partition tables or not Defaults to `by_topic`. Valid values: `by_topic`, `by_partition`, `by_topic_and_partition`.",
				Default:             stringdefault.StaticString("by_topic"),
				Validators: []validator.String{
					stringvalidator.OneOf("by_topic", "by_partition", "by_topic_and_partition"),
				},
			},
			"schema_evolution": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE` Defaults to \"basic\". Valid values: basic, none.",
				MarkdownDescription: "Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE` Defaults to `basic`. Valid values: `basic`, `none`.",
				Default:             stringdefault.StaticString("basic"),
				Validators: []validator.String{
					stringvalidator.OneOf("basic", "none"),
				},
			},
			"table_name_prefix": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Schema for the associated table name Defaults to \"streamkap\".",
				MarkdownDescription: "Schema for the associated table name Defaults to `streamkap`.",
				Default:             stringdefault.StaticString("streamkap"),
			},
			"hard_delete": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database Defaults to false.",
				MarkdownDescription: "Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database Defaults to `false`.",
				Default:             booldefault.StaticBool(false),
			},
			"tasks_max": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "The maximum number of active tasks. NOTE: Increasing this value may increase parallelism and throughput but can also lead to higher costs on databricks side. Defaults to 5.",
				MarkdownDescription: "The maximum number of active tasks. NOTE: Increasing this value may increase parallelism and throughput but can also lead to higher costs on databricks side. Defaults to `5`.",
				Default:             int64default.StaticInt64(5),
				Validators: []validator.Int64{
					int64validator.Between(1, 100),
				},
			},
			"consumer_wait_time_for_larger_batch_ms": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "The max wait time for larger batch size (in ms). The bigger the batch size, the the more cost effective loading will be on databricks but latency will grow as a trade-off. Defaults to 10000.",
				MarkdownDescription: "The max wait time for larger batch size (in ms). The bigger the batch size, the the more cost effective loading will be on databricks but latency will grow as a trade-off. Defaults to `10000`.",
				Default:             int64default.StaticInt64(10000),
				Validators: []validator.Int64{
					int64validator.Between(500, 300000),
				},
			},
			"topic2table_map": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Falls back to Streamkap's default for tables where no match is found Defaults to false.",
				MarkdownDescription: "Falls back to Streamkap's default for tables where no match is found Defaults to `false`.",
				Default:             booldefault.StaticBool(false),
			},
			"transforms_change_topic_name_match_regex": schema.StringAttribute{
				Optional:            true,
				Description:         "Regular expression for matching topic name parts to use as the destination table (database) or file (file storage) name",
				MarkdownDescription: "Regular expression for matching topic name parts to use as the destination table (database) or file (file storage) name",
			},
			"transforms_change_topic_name_mapping": schema.StringAttribute{
				Optional:            true,
				Description:         "Map source tables to specific destination tables. Input should be the format of `source_table_name:destination_table_name` separated by a new line",
				MarkdownDescription: "Map source tables to specific destination tables. Input should be the format of `source_table_name:destination_table_name` separated by a new line",
			},
		},
	}
}

// DestinationDatabricksFieldMappings maps Terraform attribute names to API field names.
var DestinationDatabricksFieldMappings = map[string]string{
	"ingestion_mode":                           "ingestion.mode",
	"databricks_token":                         "databricks.token",
	"connection_url":                           "connection.url.user.defined",
	"connection_timeout":                       "connection.timeout.user.defined",
	"databricks_catalog":                       "databricks.catalog.user.defined",
	"partition_mode":                           "partition.mode",
	"schema_evolution":                         "schema.evolution",
	"table_name_prefix":                        "table.name.prefix",
	"hard_delete":                              "hard.delete",
	"tasks_max":                                "tasks.max",
	"consumer_wait_time_for_larger_batch_ms":   "consumer.wait.time.for.larger.batch.ms",
	"topic2table_map":                          "topic2table.map.user.defined",
	"transforms_change_topic_name_match_regex": "transforms.changeTopicName.match.regex.user.defined",
	"transforms_change_topic_name_mapping":     "transforms.changeTopicName.mapping",
}
