// Code generated by tfgen. DO NOT EDIT.

package generated

import (
	"github.com/hashicorp/terraform-plugin-framework-timeouts/resource/timeouts"
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int64default"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
)

// DestinationAzblobModel is the Terraform model for the azblob destination.
type DestinationAzblobModel struct {
	ID                     types.String   `tfsdk:"id"`
	Name                   types.String   `tfsdk:"name"`
	Connector              types.String   `tfsdk:"connector"`
	AzblobConnectionString types.String   `tfsdk:"azblob_connection_string"`
	AzblobContainerName    types.String   `tfsdk:"azblob_container_name"`
	Format                 types.String   `tfsdk:"format"`
	FormatCsvWriteHeaders  types.Bool     `tfsdk:"format_csv_write_headers"`
	TopicsDir              types.String   `tfsdk:"topics_dir"`
	FileNameTemplate       types.String   `tfsdk:"file_name_template"`
	FlushSize              types.Int64    `tfsdk:"flush_size"`
	FileSize               types.Int64    `tfsdk:"file_size"`
	RotateIntervalMs       types.Int64    `tfsdk:"rotate_interval_ms"`
	Compression            types.String   `tfsdk:"compression"`
	Timeouts               timeouts.Value `tfsdk:"timeouts"`
}

// DestinationAzblobSchema returns the Terraform schema for the azblob destination.
func DestinationAzblobSchema() schema.Schema {
	return schema.Schema{
		Description: "Manages a Azure Blob Storage destination connector.",
		MarkdownDescription: "Manages a **Azure Blob Storage destination connector**.\n\n" +
			"This resource creates and manages a Azure Blob Storage destination for Streamkap data pipelines.\n\n" +
			"[Documentation](https://docs.streamkap.com/streamkap-provider-for-terraform)",
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Computed:            true,
				Description:         "Unique identifier for the destination",
				MarkdownDescription: "Unique identifier for the destination",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"name": schema.StringAttribute{
				Required:            true,
				Description:         "Name of the destination",
				MarkdownDescription: "Name of the destination",
			},
			"connector": schema.StringAttribute{
				Computed:            true,
				Description:         "Connector type",
				MarkdownDescription: "Connector type",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"azblob_connection_string": schema.StringAttribute{
				Required:            true,
				Sensitive:           true,
				Description:         "The connection string (for Account access keys) or Blob SAS URL (for Shared Access Signatures) This value is sensitive and will not appear in logs or CLI output.",
				MarkdownDescription: "The connection string (for Account access keys) or Blob SAS URL (for Shared Access Signatures)\n\n**Security:** This value is marked sensitive and will not appear in CLI output or logs.",
			},
			"azblob_container_name": schema.StringAttribute{
				Optional:            true,
				Description:         "The name of an existing blob container to use",
				MarkdownDescription: "The name of an existing blob container to use",
			},
			"format": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "The format to use when writing data to file storage Defaults to \"json\". Valid values: json, csv, avro, parquet.",
				MarkdownDescription: "The format to use when writing data to file storage Defaults to `json`. Valid values: `json`, `csv`, `avro`, `parquet`.",
				Default:             stringdefault.StaticString("json"),
				Validators: []validator.String{
					stringvalidator.OneOf("json", "csv", "avro", "parquet"),
				},
			},
			"format_csv_write_headers": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "Include or exclude column name header row per file Defaults to false.",
				MarkdownDescription: "Include or exclude column name header row per file Defaults to `false`.",
				Default:             booldefault.StaticBool(false),
			},
			"topics_dir": schema.StringAttribute{
				Optional:            true,
				Description:         "Top level directory for storing the data e.g. myfolder/subfolder",
				MarkdownDescription: "Top level directory for storing the data e.g. myfolder/subfolder",
			},
			"file_name_template": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				Description:         "The format of the filename. See documentation for more information about formatting options. Defaults to \"{{topic}}-{{partition}}-{{start_offset}}\".",
				MarkdownDescription: "The format of the filename. See documentation for more information about formatting options. Defaults to `{{topic}}-{{partition}}-{{start_offset}}`.",
				Default:             stringdefault.StaticString("{{topic}}-{{partition}}-{{start_offset}}"),
			},
			"flush_size": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "Number of records to write per file Defaults to 1000.",
				MarkdownDescription: "Number of records to write per file Defaults to `1000`.",
				Default:             int64default.StaticInt64(1000),
			},
			"file_size": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "Minimum size (in bytes) per file. Records are held in memory until this file size is met or the **Rotate interval** is exceeded Defaults to 65536.",
				MarkdownDescription: "Minimum size (in bytes) per file. Records are held in memory until this file size is met or the **Rotate interval** is exceeded Defaults to `65536`.",
				Default:             int64default.StaticInt64(65536),
			},
			"rotate_interval_ms": schema.Int64Attribute{
				Optional:            true,
				Computed:            true,
				Description:         "Maximum time (in milliseconds) to wait before writing records held in memory to file. This ignores the flush and file size settings Defaults to -1.",
				MarkdownDescription: "Maximum time (in milliseconds) to wait before writing records held in memory to file. This ignores the flush and file size settings Defaults to `-1`.",
				Default:             int64default.StaticInt64(-1),
			},
			"compression": schema.StringAttribute{
				Optional:            true,
				Description:         "The compression type to use when writing data to the storage.",
				MarkdownDescription: "The compression type to use when writing data to the storage.",
			},
		},
	}
}

// DestinationAzblobFieldMappings maps Terraform attribute names to API field names.
var DestinationAzblobFieldMappings = map[string]string{
	"azblob_connection_string": "azblob.connection.string.user.defined",
	"azblob_container_name":    "azblob.container.name",
	"format":                   "format.user.defined",
	"format_csv_write_headers": "format.csv.write.headers",
	"topics_dir":               "topics.dir",
	"file_name_template":       "file.name.template",
	"flush_size":               "flush.size",
	"file_size":                "file.size",
	"rotate_interval_ms":       "rotate.interval.ms",
	"compression":              "compression",
}
