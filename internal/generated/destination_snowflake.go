// Code generated by tfgen. DO NOT EDIT.

package generated

import (
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/booldefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringdefault"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
)

// DestinationSnowflakeModel is the Terraform model for the snowflake destination.
type DestinationSnowflakeModel struct {
	ID                                   types.String `tfsdk:"id"`
	Name                                 types.String `tfsdk:"name"`
	Connector                            types.String `tfsdk:"connector"`
	SnowflakeURLName                     types.String `tfsdk:"snowflake_url_name"`
	SnowflakeUserName                    types.String `tfsdk:"snowflake_user_name"`
	SnowflakePrivateKey                  types.String `tfsdk:"snowflake_private_key"`
	SnowflakePrivateKeyPassphraseSecured types.Bool   `tfsdk:"snowflake_private_key_passphrase_secured"`
	SnowflakePrivateKeyPassphrase        types.String `tfsdk:"snowflake_private_key_passphrase"`
	Sfwarehouse                          types.String `tfsdk:"sfwarehouse"`
	SnowflakeDatabaseName                types.String `tfsdk:"snowflake_database_name"`
	SnowflakeSchemaName                  types.String `tfsdk:"snowflake_schema_name"`
	CreateSchemaAuto                     types.Bool   `tfsdk:"create_schema_auto"`
	SnowflakeRoleName                    types.String `tfsdk:"snowflake_role_name"`
	IngestionMode                        types.String `tfsdk:"ingestion_mode"`
	HardDelete                           types.Bool   `tfsdk:"hard_delete"`
	SchemaEvolution                      types.String `tfsdk:"schema_evolution"`
	UseHybridTables                      types.Bool   `tfsdk:"use_hybrid_tables"`
	ApplyDynamicTableScript              types.Bool   `tfsdk:"apply_dynamic_table_script"`
	CreateSqlExecute                     types.String `tfsdk:"create_sql_execute"`
	SqlTableName                         types.String `tfsdk:"sql_table_name"`
	CreateSqlData                        types.String `tfsdk:"create_sql_data"`
	AutoQaDedupeTableMapping             types.String `tfsdk:"auto_qa_dedupe_table_mapping"`
	SnowflakeTopic2tableMap              types.String `tfsdk:"snowflake_topic2table_map"`
}

// DestinationSnowflakeSchema returns the Terraform schema for the snowflake destination.
func DestinationSnowflakeSchema() schema.Schema {
	return schema.Schema{
		MarkdownDescription: "Snowflake destination connector",
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Computed:            true,
				MarkdownDescription: "Unique identifier for the destination",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"name": schema.StringAttribute{
				Required:            true,
				MarkdownDescription: "Name of the destination",
			},
			"connector": schema.StringAttribute{
				Computed:            true,
				MarkdownDescription: "Connector type",
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"snowflake_url_name": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "The URL for accessing your Snowflake account. This URL must include your account identifier. Note that the protocol (https://) and port number are optional.",
			},
			"snowflake_user_name": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "User login name for the Snowflake account.\n\n",
			},
			"snowflake_private_key": schema.StringAttribute{
				Optional:            true,
				Sensitive:           true,
				MarkdownDescription: "The private key to authenticate the user. Include only the key, not the header or footer. If the key is split across multiple lines, remove the line breaks.",
			},
			"snowflake_private_key_passphrase_secured": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "If checked (default), provide your SSH key's passphrase, otherwise, uncheck for SSH keys without passphrase.",
				Default:             booldefault.StaticBool(true),
			},
			"snowflake_private_key_passphrase": schema.StringAttribute{
				Optional:            true,
				Sensitive:           true,
				MarkdownDescription: "The passphrase is used to decrypt the private key.",
			},
			"sfwarehouse": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "The name of the snowflake warehouse.",
				Default:             stringdefault.StaticString("STREAMKAP_WH"),
			},
			"snowflake_database_name": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "The name of the database that contains the table to insert rows into.",
			},
			"snowflake_schema_name": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "The name of the schema that contains the table to insert rows into.",
			},
			"create_schema_auto": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Automatically generates a Snowflake schema if it does not already exist.",
				Default:             booldefault.StaticBool(true),
			},
			"snowflake_role_name": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "The name of an existing role with necessary privileges (for Streamkap) assigned to the <Username>",
				Default:             stringdefault.StaticString("STREAMKAP_ROLE"),
			},
			"ingestion_mode": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "<span>Upsert or append modes are available. NOTE: when switching append to upsert, existing data must be deduplicated or deleted. <a href='https://docs.streamkap.com/docs/snowflake#upsert-mode' class='docs-url' target='_blank'>Read more about upsert mode</a> </span>",
				Default:             stringdefault.StaticString("append"),
				Validators: []validator.String{
					stringvalidator.OneOf("upsert", "append"),
				},
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.RequiresReplace(),
				},
			},
			"hard_delete": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database (applies to `upsert` only)",
				Default:             booldefault.StaticBool(true),
			},
			"schema_evolution": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE`",
				Default:             stringdefault.StaticString("basic"),
				Validators: []validator.String{
					stringvalidator.OneOf("basic", "none"),
				},
			},
			"use_hybrid_tables": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Specifies whether the connector should create Hybrid Tables (applies to `upsert` only)",
				Default:             booldefault.StaticBool(false),
			},
			"apply_dynamic_table_script": schema.BoolAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Specifies whether the connector should create Dynamic Tables & Cleanup Tasks (applies to `append` only)",
				Default:             booldefault.StaticBool(false),
			},
			"create_sql_execute": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "These template queries run for each table the first time a record is streamed for them.",
				Default:             stringdefault.StaticString("CREATE OR REPLACE DYNAMIC TABLE {{table}}_DT TARGET_LAG='15 minutes' WAREHOUSE={{warehouse}} AS SELECT * EXCLUDE dedupe_id FROM( SELECT *, ROW_NUMBER() OVER (PARTITION BY {{primaryKeyColumns}} ORDER BY _streamkap_ts_ms DESC, _streamkap_offset DESC) AS dedupe_id FROM {{table}} ) WHERE dedupe_id = 1 AND __deleted = 'false';\nCREATE OR REPLACE TASK {{table}}_CT WAREHOUSE={{warehouse}} SCHEDULE='4380 minutes' TASK_AUTO_RETRY_ATTEMPTS=3 ALLOW_OVERLAPPING_EXECUTION=FALSE AS DELETE FROM {{table}} WHERE NOT EXISTS ( SELECT 1 FROM ( SELECT {{primaryKeyColumns}}, MAX(_streamkap_ts_ms) AS max_timestamp FROM {{table}} GROUP BY {{primaryKeyColumns}} ) AS subquery WHERE {{{keyColumnsAndCondition}}} AND {{table}}._streamkap_ts_ms = subquery.max_timestamp);\nALTER TASK {{table}}_CT RESUME"),
			},
			"sql_table_name": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Can be used as <code>{{dynamicTableName}}</code> in dynamic table creation SQL. It can use input JSON data for more complex mappings and logic.",
				Default:             stringdefault.StaticString("{{table}}_DT"),
			},
			"create_sql_data": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "Use <code>{\"TABLE_DATA\": {\"{table_name}\": {\"{key}\": \"{value}\"}, ...}, ...}</code> to set table specific data. This data will be available in the custom SQL templates e.g. <code>SELECT {{key}}</code>.",
			},
			"auto_qa_dedupe_table_mapping": schema.StringAttribute{
				Optional:            true,
				MarkdownDescription: "Mapping between the tables that store append-only data and the deduplicated tables. The dedupeTable in mapping will be used for QA scripts. If dedupeSchema is not specified, the deduplicated table will be created in the same schema as the raw table.",
			},
			"snowflake_topic2table_map": schema.StringAttribute{
				Optional:            true,
				Computed:            true,
				MarkdownDescription: "Define custom topic-to-table name mapping using regex. Format: <code>matching_pattern:replacement_pattern</code>. Use $1, $2, etc. for captured groups. Example: <code>^([-\\w]+\\.)([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+):$5</code> uses only the last segment as table name.",
				Default:             stringdefault.StaticString("^([-\\w]+\\.)([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+):$5"),
			},
		},
	}
}

// DestinationSnowflakeFieldMappings maps Terraform attribute names to API field names.
var DestinationSnowflakeFieldMappings = map[string]string{
	"snowflake_url_name":                       "snowflake.url.name",
	"snowflake_user_name":                      "snowflake.user.name",
	"snowflake_private_key":                    "snowflake.private.key",
	"snowflake_private_key_passphrase_secured": "snowflake.private.key.passphrase.secured",
	"snowflake_private_key_passphrase":         "snowflake.private.key.passphrase",
	"sfwarehouse":                              "sfwarehouse",
	"snowflake_database_name":                  "snowflake.database.name",
	"snowflake_schema_name":                    "snowflake.schema.name",
	"create_schema_auto":                       "create.schema.auto",
	"snowflake_role_name":                      "snowflake.role.name",
	"ingestion_mode":                           "ingestion.mode",
	"hard_delete":                              "hard.delete",
	"schema_evolution":                         "schema.evolution",
	"use_hybrid_tables":                        "use.hybrid.tables",
	"apply_dynamic_table_script":               "apply.dynamic.table.script",
	"create_sql_execute":                       "create.sql.execute",
	"sql_table_name":                           "sql.table.name",
	"create_sql_data":                          "create.sql.data",
	"auto_qa_dedupe_table_mapping":             "auto.qa.dedupe.table.mapping",
	"snowflake_topic2table_map":                "snowflake.topic2table.map",
}
