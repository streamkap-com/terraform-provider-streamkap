---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "streamkap_source Resource - terraform-provider-streamkap"
subcategory: ""
description: |-
  Source MySQL resource
---

# streamkap_source (Resource)

Source MySQL resource

<!-- schema generated by tfplugindocs -->

## Schema

### Required

- `config` (String) Source config (Json String)
- `connector` (String) Source connector
- `name` (String) The name of the source

<!-- Example -->

## Example Usage

### PostgreSQL Connector

```terraform
data "streamkap_token" "this" {}
resource "streamkap_source" "postgresql" {
  name      = "My Postgres"
  connector = "postgresql"
  config = jsonencode({
    "database.hostname.user.defined"            = "localhost"
    "database.port"                             = "5432"
    "database.user"                             = "postgres"
    "database.password"                         = "my_password"
    "signal.data.collection.schema.or.database" = "public"
    "database.dbname"                           = "my_database"
    "schema.include.list"                       = "schema1, schema2"
    "table.include.list.user.defined"           = "schema1.table1, schema2.table2, schema2.table3"
    "slot.name"                                 = "streamkap_pgoutput_slot"
    "publication.name"                          = "streamkap_pub"
    "database.sslmode"                          = "require"
    "snapshot.max.threads"                      = "1"
    "snapshot.fetch.size"                       = "102400"
    "snapshot.mode.user.defined"                = "Initial"
    "binary.handling.mode"                      = "bytes"
    "incremental.snapshot.chunk.size"           = 102400
    "max.batch.size"                            = 2048
    "max.queue.size.user.defined"               = "204800"
  })
}
```

#### Argument Reference

- `config` - (Required) Source config (Json String)
- `connector` (Required) Set the value as `postgresql` if we want to create a PostgresQL conenctor
- `name` (Required) Source name

##### "config" object

- `database.hostname.user.defined` - (Required) PostgreSQL Hostname. For example, postgres.something.rds.amazonaws.com
- `database.port` (Required) PostgreSQL Port. For example, 5432
- `database.user` (Required) Username to access the database
- `database.password` (Required) Password to access the database
- `signal.data.collection.schema.or.database` (Required) Streamkap will use a table in this schema to monitor incremental snapshotting. Follow the instructions in the documentation for creating this table and specify which schema to use here
- `database.dbname` (Required) Database from which to stream data
- `schema.include.list` (Required) Comma-separated list of schemas to include in the stream.
- `table.include.list.user.defined` (Required) Comma-separated list of source tables to sync
- `slot.name` (Required) The name of the replication slot for the connector to use.
- `publication.name` (Required) The name of the publication for the connector to use.
- `database.sslmode` (Required) Whether to use an encrypted connection to the PostgreSQL server. Options: `required` and `disable`
- `snapshot.max.threads` (Required) How many tables to snapshot at the same time during initial snapshots. Min: `102400`, Max: `512000`
- `snapshot.fetch.size` (Required) How many rows to fetch in batches during initial snapshots. Min: `102400`, Max: `512000`
- `snapshot.mode.user.defined` (Required) Select the snapshot mode to use. 'Initial' is recommended and will snapshot existing data before streaming records. Options: `Initial` and `Never`
- `binary.handling.mode` (Required) Specifies how the data for binary columns e.g. blob, binary, varbinary should be represented. This setting depends on what the destination is. See the documentation for more details. Options: `bytes`, `base64`, `hex` and `base64-url-safe`
- `incremental.snapshot.chunk.size` (Required) How many rows to fetch in batches during incremental snapshots. Min: `102400`, Max: `512000`
- `max.batch.size` (Required) Maximum size of each batch of source records. Min: `2048`, Max: `10240`
- `max.queue.size.user.defined` (Required) Maximum number of pre-fetched records, awaiting to be polled. Max Queue Size must always be greater than 125% of Snapshot Rows (Initial). Min: `204800`, Max: `1024000`

### MySQL Connector

```terraform
data "streamkap_token" "this" {}
resource "streamkap_source" "mysql" {
  name      = "MySQL"
  connector = "mysql"
  config = jsonencode({
    "database.hostname.user.defined"     = "localhost"
    "database.port"                      = "3306"
    "database.user"                      = "root"
    "database.password"                  = "password"
    "database.include.list.user.defined" = "database1, database2"
    "table.include.list.user.defined"    = "database1.table1, database2.table2",
    "database.connectionTimeZone"        = "SERVER",
    "database.connectionTimeZone"        = "SERVER"
    "snapshot.gtid"                      = "Yes"
    "snapshot.mode.user.defined"         = "When Needed"
    "binary.handling.mode"               = "bytes"
    "snapshot.max.threads"               = "1"
    "snapshot.fetch.size"                = "102400"
    "incremental.snapshot.chunk.size"    = 102400
    "incremental.snapshot.chunk.size"    = 1024
    "max.batch.size"                     = 2048
  })
}
```

#### Argument Reference

- `config` - (Required) Source config (Json String)
- `connector` (Required) Set the value as `mysql` if we want to create a MySQL conenctor
- `name` (Required) Source name

##### "config" object

- `database.hostname.user.defined` - (Required) MySQL Hostname. For example, mysqldb.something.rds.amazonaws.com
- `database.port` (Required) MySQL Port. For example, 5432
- `database.user` (Required) Username to access the database
- `database.password` (Required) Password to access the database
- `database.include.list.user.defined` (Required) Comma-separated list of schemas to include in the stream.
- `table.include.list.user.defined` (Required) Comma-separated list of source tables to sync
- `database.connectionTimeZone` (Required) Set the connection timezone. If set to SERVER, the source will detect the connection time zone from the values configured on the MySQL server session variables 'time_zone' or 'system_time_zone'. Options: `SERVER`, `UTC`, `Africa/Cairo`, ` Asia/Riyadh`, `Africa/Casablanca`, `Asia/Seoul`, `Africa/Harare`, `Asia/Shanghai`, `Africa/Monrovia`, `Asia/Singapore`, `Africa/Nairobi`, `Asia/Taipei`, `Africa/Tripoli`, `Asia/Tehran`, `Africa/Windhoek`, `Asia/Tokyo`, `America/Araguaina`, `Asia/Ulaanbaatar`, `America/Asuncion`, `Asia/Vladivostok`, `America/Bogota`, `Asia/Yakutsk`, `America/Buenos_Aires`, `Asia/Yerevan`, `America/Caracas`, `Atlantic/Azores`, `America/Chihuahua`, `Australia/Adelaide`, `America/Cuiaba`, `Australia/Brisbane`, `America/Denver`, `Australia/Darwin`, `America/Fortaleza`, `Australia/Hobart`, `America/Guatemala`, `Australia/Perth`, `America/Halifax`, `Australia/Sydney`, `America/Manaus`, `Brazil/East`, `America/Matamoros`, `Canada/Newfoundland`, `America/Monterrey`, `Canada/Saskatchewan`, `America/Montevideo`, `Canada/Yukon`, `America/Phoenix`, `Europe/Amsterdam`, `America/Santiago`, `Europe/Athens`, `America/Tijuana`, `Europe/Dublin`, `Asia/Amman`, `Europe/Helsinki`, `Asia/Ashgabat`, `Europe/Istanbul`, `Asia/Baghdad`, `Europe/Kaliningrad`, `Asia/Baku`, `Europe/Moscow`, `Asia/Bangkok`, `Europe/Paris`, `Asia/Beirut`, `Europe/Prague`, `Asia/Calcutta`, `Europe/Sarajevo`, `Asia/Damascus`, `Pacific/Auckland`, `Asia/Dhaka`, `Pacific/Fiji`, `Asia/Irkutsk`, `Pacific/Guam`, `Asia/Jerusalem`, `Pacific/Honolulu`, `Asia/Kabul`, `Pacific/Samoa`, `Asia/Karachi`, `US/Alaska`, `Asia/Kathmandu`, `US/Central`, `Asia/Krasnoyarsk`, `US/Eastern`, `Asia/Magadan`, `US/East-Indiana`, `Asia/Muscat`, `US/Pacific` and `Asia/Novosibirsk`.
- `snapshot.gtid` (Required) GTID snapshots are read only but require some prerequisite settings, including enabling GTID on the source database. See the documentation for more details. Options: `Yes` and `No`
- `snapshot.max.threads` (Required) How many tables to snapshot at the same time during initial snapshots. Min: `102400`, Max: `512000`
- `snapshot.fetch.size` (Required) How many rows to fetch in batches during initial snapshots. Min: `102400`, Max: `512000`
- `snapshot.mode.user.defined` (Required) Select the snapshot mode to use. 'When Needed' is recommended and will snapshot existing data before streaming records. Options: `Initial`, `Never`, `When Needed`, `Schema Only` and `Schema Only Recovery`
- `binary.handling.mode` (Required) Specifies how the data for binary columns e.g. blob, binary, varbinary should be represented. This setting depends on what the destination is. See the documentation for more details. Options: `bytes`, `base64`, `hex` and `base64-url-safe`
- `incremental.snapshot.chunk.size` (Required) How many rows to fetch in batches during incremental snapshots. Min: `102400`, Max: `512000`
- `max.batch.size` (Required) Maximum size of each batch of source records. Min: `2048`, Max: `10240`
- `max.queue.size.user.defined` (Required) Maximum number of pre-fetched records, awaiting to be polled. Max Queue Size must always be greater than 125% of Snapshot Rows (Initial). Min: `204800`, Max: `1024000`
