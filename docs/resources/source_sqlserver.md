---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "streamkap_source_sqlserver Resource - terraform-provider-streamkap"
subcategory: ""
description: |-
  Source SQLServer resource
---

# streamkap_source_sqlserver (Resource)

Source SQLServer resource

## Example Usage

```terraform
terraform {
  required_providers {
    streamkap = {
      source  = "streamkap-com/streamkap"
      version = ">= 2.0.0"
    }
  }
  required_version = ">= 1.0.0"
}

provider "streamkap" {}

variable "source_sqlserver_hostname" {
  type        = string
  description = "The hostname of the SQLServer database"
}
variable "source_sqlserver_password" {
  type        = string
  sensitive   = true
  description = "The password of the SQLServer database"
}

resource "streamkap_source_sqlserver" "example-source-sqlserver" {
  name                                         = "example-source-sqlserver"
  database_hostname                            = var.source_sqlserver_hostname
  database_port                                = 1433
  database_user                                = "admin"
  database_password                            = var.source_sqlserver_password
  database_dbname                              = "sqlserverdemo"
  schema_include_list                          = "dbo"
  table_include_list                           = "dbo.Orders"
  signal_data_collection_schema_or_database    = "streamkap"
  heartbeat_enabled                            = false
  heartbeat_data_collection_schema_or_database = null
  binary_handling_mode                         = "bytes"
  ssh_enabled                                  = false
  insert_static_key_field                      = "key_field"
  insert_static_key_value                      = "key_value"
  insert_static_value_field                    = "value_field"
  insert_static_value                          = "value_value"
  snapshot_parallelism                         = 2
  snapshot_large_table_threshold               = 12000
  snapshot_custom_table_config = {
    "dbo.Orders" = {
      chunks = 2
    }
  }

}

output "example-source-sqlserver" {
  value = streamkap_source_sqlserver.example-source-sqlserver.id
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `database_dbname` (String) Source Databases
- `database_hostname` (String) SQLServer Hostname. For example, sqlserverdb.something.rds.amazonaws.com
- `database_password` (String, Sensitive) Password to access the database
- `database_user` (String) Username to access the database
- `name` (String) Source name
- `schema_include_list` (String) Source schemas to sync
- `table_include_list` (String) Source tables to sync

### Optional

- `binary_handling_mode` (String) Specifies how the data for binary columns e.g. blob, binary, varbinary should be represented. This setting depends on what the destination is. See the documentation for more details.
- `column_exclude_list` (String) Comma separated list of columns blacklist regular expressions, format schema[.]table[.](column1|column2|etc)
- `database_port` (Number) SQLServer Port. For example, 1433
- `heartbeat_data_collection_schema_or_database` (String) Heartbeat Table Database
- `heartbeat_enabled` (Boolean) Heartbeats are used to keep the pipeline healthy when there is a low volume of data at times.
- `insert_static_key_field` (String) The name of the static field to be added to the message key.
- `insert_static_key_value` (String) The value of the static field to be added to the message key.
- `insert_static_value` (String) The value of the static field to be added to the message value.
- `insert_static_value_field` (String) The name of the static field to be added to the message value.
- `poll_interval_ms` (Number) The number of milliseconds the connector waits for new change events to appear before processing a batch.
- `signal_data_collection_schema_or_database` (String) Schema for signal data collection. If connector is in read-only mode (snapshot_gtid="Yes"), set this to null.
- `snapshot_custom_table_config` (Attributes Map) Explicitly set nb of parallel chunks for tables. Format: {"db.Some_Tbl": {"chunks": 5}}. This allows manual settings for parallelization when stats are outdated and estimated table size cannot be computed reliably (see [below for nested schema](#nestedatt--snapshot_custom_table_config))
- `snapshot_large_table_threshold` (Number) The threshold in MB for a Large Table to require multiple chunks to be read in parallel
- `snapshot_parallelism` (Number) How many parallel chunk requests to send to the source DB
- `ssh_enabled` (Boolean) Connect via SSH tunnel
- `ssh_host` (String) Hostname of the SSH server, only required if `ssh_enabled` is true
- `ssh_port` (String) Port of the SSH server, only required if `ssh_enabled` is true
- `ssh_user` (String) User for connecting to the SSH server, only required if `ssh_enabled` is true

### Read-Only

- `connector` (String)
- `id` (String) Source SQLServer identifier

<a id="nestedatt--snapshot_custom_table_config"></a>
### Nested Schema for `snapshot_custom_table_config`

Required:

- `chunks` (Number)

## Import

Import is supported using the following syntax:

```shell
# Source SQL Server can be imported by specifying the identifier
terraform import streamkap_source_sqlserver.example-source-sqlserver 686dfd294321ea8a67d6d190
```
