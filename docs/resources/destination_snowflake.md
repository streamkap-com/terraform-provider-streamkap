---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "streamkap_destination_snowflake Resource - terraform-provider-streamkap"
subcategory: ""
description: |-
  Destination Snowflake resource
---

# streamkap_destination_snowflake (Resource)

Destination Snowflake resource

## Example Usage

```terraform
terraform {
  required_providers {
    streamkap = {
      source  = "streamkap-com/streamkap"
      version = ">= 2.0.0"
    }
  }
  required_version = ">= 1.0.0"
}

provider "streamkap" {}

variable "destination_snowflake_url_name" {
  type        = string
  description = "The URL name of the Snowflake database"
}
variable "destination_snowflake_private_key" {
  type        = string
  sensitive   = true
  description = "The private key of the Snowflake database"
}
variable "destination_snowflake_key_passphrase" {
  type        = string
  sensitive   = true
  description = "The passphrase of the private key of the Snowflake database"
}
resource "streamkap_destination_snowflake" "example-destination-snowflake" {
  name                             = "example-destination-snowflake"
  snowflake_url_name               = var.destination_snowflake_url_name
  snowflake_user_name              = "STREAMKAP_USER_JUNIT"
  snowflake_private_key            = var.destination_snowflake_private_key
  snowflake_private_key_passphrase = var.destination_snowflake_key_passphrase
  sfwarehouse                      = "STREAMKAP_WH"
  snowflake_database_name          = "JUNIT"
  snowflake_schema_name            = "JUNIT"
  snowflake_role_name              = "STREAMKAP_ROLE_JUNIT"
  ingestion_mode                   = "upsert"
  hard_delete                      = true
  use_hybrid_tables                = false
  apply_dynamic_table_script       = false
  snowflake_topic2table_map        = "REGEX_MATCHER>^([-\\w]+\\.)([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+\\.)?([-\\w]+):$5"
  auto_qa_dedupe_table_mapping = {
    users                   = "JUNIT.USERS",
    itst_scen20240528103635 = "ITST_SCEN20240528103635"
  }
}

output "example-destination-snowflake" {
  value = streamkap_destination_snowflake.example-destination-snowflake.id
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `name` (String) Destination name
- `snowflake_database_name` (String) The name of the database that contains the table to insert rows into.
- `snowflake_private_key` (String, Sensitive) The private key to authenticate the user. Include only the key, not the header or footer. If the key is split across multiple lines, remove the line breaks.
- `snowflake_schema_name` (String) The name of the schema that contains the table to insert rows into.
- `snowflake_url_name` (String) The URL for accessing your Snowflake account. This URL must include your account identifier. Note that the protocol (https://) and port number are optional.
- `snowflake_user_name` (String) User login name for the Snowflake account.

### Optional

- `apply_dynamic_table_script` (Boolean) Specifies whether the connector should create Dyanmic Tables & Cleanup Task (applies to `append` mode only)
- `auto_qa_dedupe_table_mapping` (Map of String) Mapping between the tables that store append-only data and the deduplicated tables, e.g. rawTable1:[dedupeSchema.]dedupeTable1,rawTable2:[dedupeSchema.]dedupeTable2,etc. The dedupeTable in mapping will be used for QA scripts. If dedupeSchema is not specified, the deduplicated table will be created in the same schema as the raw table.
- `auto_schema_creation` (Boolean) Specifies whether the connector should create the schema automatically. If set to `false`, the schema must be created manually before starting the connector.
- `create_sql_data` (String) Custom SQL mustache template input JSON data. Use TABLE_DATA dictionary to set table specific data. e.g:
	```
	{
	    "TABLE_DATA": {
	        "my-table-name": {
	            "someTableSpecificKey": "someTableSpecificValue"
	        }
	    }
	}
	```
- `create_sql_execute` (String) Custom SQL mustache template to be run the first time a record is streamed for each table. Default is: 
	```
	CREATE OR REPLACE DYNAMIC TABLE {{table}}_DT TARGET_LAG='15 minutes' WAREHOUSE={{warehouse}} AS SELECT * EXCLUDE dedupe_id FROM( SELECT *, ROW_NUMBER() OVER (PARTITION BY {{primaryKeyColumns}} ORDER BY _streamkap_ts_ms DESC, _streamkap_offset DESC) AS dedupe_id FROM {{table}} ) WHERE dedupe_id = 1 AND __deleted = 'false';
	CREATE OR REPLACE TASK {{table}}_CT WAREHOUSE={{warehouse}} SCHEDULE='4380 minutes' TASK_AUTO_RETRY_ATTEMPTS=3 ALLOW_OVERLAPPING_EXECUTION=FALSE AS DELETE FROM {{table}} WHERE NOT EXISTS ( SELECT 1 FROM ( SELECT {{primaryKeyColumns}}, MAX(_streamkap_ts_ms) AS max_timestamp FROM {{table}} GROUP BY {{primaryKeyColumns}} ) AS subquery WHERE {{{keyColumnsAndCondition}}} AND {{table}}._streamkap_ts_ms = subquery.max_timestamp);
	ALTER TASK {{table}}_CT RESUME
	```
- `hard_delete` (Boolean) Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database (applies to `upsert` only)
- `ingestion_mode` (String) `upsert` or `append` modes are available
- `schema_evolution` (String) Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `none`
- `sfwarehouse` (String) The name of the Snowflake warehouse.
- `snowflake_private_key_passphrase` (String, Sensitive) If the value is not empty, this phrase is used to try to decrypt the private key.
- `snowflake_role_name` (String) The name of an existing role with necessary privileges (for Streamkap) assigned to the Username.
- `snowflake_topic2table_map` (String) Define custom topic-to-table name mapping using regex. Format: <code>matching_pattern:replacement_pattern</code>. Use $1, $2, etc. for captured groups. Example: <code>REGEX_MATCHER>^([-\w]+\.)([-\w]+\.)?([-\w]+\.)?([-\w]+\.)?([-\w]+):$5</code> uses only the last segment as table name
- `sql_table_name` (String) Dynamic Table Name mustache template. Can be used as `{{dynamicTableName}}` in dynamic table creation SQL. It can use input JSON data for more complex mappings and logic.
- `use_hybrid_tables` (Boolean) Specifies whether the connector should create Hybrid Tables (applies to `upsert` only)

### Read-Only

- `connector` (String)
- `id` (String) Destination Snowflake identifier

## Import

Import is supported using the following syntax:

```shell
# Destination Snowflake can be imported by specifying the identifier.
terraform import streamkap_destination_snowflake.example-destination-snowflake 665e894ebb3753f38d983cee
```
