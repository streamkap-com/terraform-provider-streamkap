---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "streamkap_destination_snowflake Resource - terraform-provider-streamkap"
subcategory: ""
description: |-
  Manages a Snowflake destination connector.
  This resource creates and manages a Snowflake destination for Streamkap data pipelines.
  Documentation https://docs.streamkap.com/streamkap-provider-for-terraform
---

# streamkap_destination_snowflake (Resource)

Manages a **Snowflake destination connector**.

This resource creates and manages a Snowflake destination for Streamkap data pipelines.

[Documentation](https://docs.streamkap.com/streamkap-provider-for-terraform)



<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `name` (String) Name of the destination

### Optional

- `apply_dynamic_table_script` (Boolean) Specifies whether the connector should create Dynamic Tables & Cleanup Tasks. **Conditionally required** when `ingestion_mode` is `append`. Defaults to `false`.
- `auto_qa_dedupe_table_mapping` (Map of String) Mapping between the tables that store append-only data and the deduplicated tables, e.g. rawTable1:[dedupeSchema.]dedupeTable1,rawTable2:[dedupeSchema.]dedupeTable2,etc. The dedupeTable in mapping will be used for QA scripts. If dedupeSchema is not specified, the deduplicated table will be created in the same schema as the raw table. Conditionally required when ingestion_mode is 'append' and apply_dynamic_table_script is true.
- `auto_schema_creation` (Boolean, Deprecated) DEPRECATED: Use 'create_schema_auto' instead.
- `create_schema_auto` (Boolean) Automatically generates a Snowflake schema if it does not already exist. Defaults to `true`.
- `create_sql_data` (String) Use <code>{"TABLE_DATA": {"{table_name}": {"{key}": "{value}"}, ...}, ...}</code> to set table specific data. This data will be available in the custom SQL templates e.g. <code>SELECT {{key}}</code>. Conditionally required when ingestion_mode is 'append' and apply_dynamic_table_script is true.
- `create_sql_execute` (String) These template queries run for each table the first time a record is streamed for them. **Conditionally required** when `ingestion_mode` is `append` and `apply_dynamic_table_script` is `true`. Defaults to `CREATE OR REPLACE DYNAMIC TABLE {{table}}_DT TARGET_LAG='15 minutes' WAREHOUSE={{warehouse}} AS SELECT * EXCLUDE dedupe_id FROM( SELECT *, ROW_NUMBER() OVER (PARTITION BY {{primaryKeyColumns}} ORDER BY _streamkap_ts_ms DESC, _streamkap_offset DESC) AS dedupe_id FROM {{table}} ) WHERE dedupe_id = 1 AND __deleted = 'false';
CREATE OR REPLACE TASK {{table}}_CT WAREHOUSE={{warehouse}} SCHEDULE='4380 minutes' TASK_AUTO_RETRY_ATTEMPTS=3 ALLOW_OVERLAPPING_EXECUTION=FALSE AS DELETE FROM {{table}} WHERE NOT EXISTS ( SELECT 1 FROM ( SELECT {{primaryKeyColumns}}, MAX(_streamkap_ts_ms) AS max_timestamp FROM {{table}} GROUP BY {{primaryKeyColumns}} ) AS subquery WHERE {{{keyColumnsAndCondition}}} AND {{table}}._streamkap_ts_ms = subquery.max_timestamp);
ALTER TASK {{table}}_CT RESUME`.
- `hard_delete` (Boolean) Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database. **Conditionally required** when `ingestion_mode` is `upsert`. Defaults to `true`.
- `ingestion_mode` (String) <span>Upsert or append modes are available. NOTE: when switching append to upsert, existing data must be deduplicated or deleted. <a href='https://docs.streamkap.com/docs/snowflake#upsert-mode' class='docs-url' target='_blank'>Read more about upsert mode</a> </span> Defaults to `append`. Valid values: `upsert`, `append`.
- `schema_evolution` (String) Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE`. **Conditionally required** when `ingestion_mode` is `upsert`. Defaults to `basic`. Valid values: `basic`, `none`.
- `sfwarehouse` (String) The name of the snowflake warehouse. Defaults to `STREAMKAP_WH`.
- `snowflake_database_name` (String) The name of the database that contains the table to insert rows into.
- `snowflake_private_key` (String, Sensitive) The private key to authenticate the user. Include only the key, not the header or footer. If the key is split across multiple lines, remove the line breaks.

**Security:** This value is marked sensitive and will not appear in CLI output or logs.
- `snowflake_private_key_passphrase` (String, Sensitive) The passphrase is used to decrypt the private key. **Conditionally required** when `snowflake_private_key_passphrase_secured` is `true`.

**Security:** This value is marked sensitive and will not appear in CLI output or logs.
- `snowflake_private_key_passphrase_secured` (Boolean) If checked (default), provide your SSH key's passphrase, otherwise, uncheck for SSH keys without passphrase. Defaults to `true`.
- `snowflake_role_name` (String) The name of an existing role with necessary privileges (for Streamkap) assigned to the <Username> Defaults to `STREAMKAP_ROLE`.
- `snowflake_schema_name` (String) The name of the schema that contains the table to insert rows into.
- `snowflake_topic2table_map` (String) Define custom topic-to-table name mapping using regex. Format: <code>matching_pattern:replacement_pattern</code>. Use $1, $2, etc. for captured groups. Example: <code>^([-\w]+\.)([-\w]+\.)?([-\w]+\.)?([-\w]+\.)?([-\w]+):$5</code> uses only the last segment as table name. Defaults to `^([-\w]+\.)([-\w]+\.)?([-\w]+\.)?([-\w]+\.)?([-\w]+):$5`.
- `snowflake_url_name` (String) The URL for accessing your Snowflake account. This URL must include your account identifier. Note that the protocol (https://) and port number are optional.
- `snowflake_user_name` (String) User login name for the Snowflake account.
- `sql_table_name` (String) Can be used as <code>{{dynamicTableName}}</code> in dynamic table creation SQL. It can use input JSON data for more complex mappings and logic. **Conditionally required** when `ingestion_mode` is `append` and `apply_dynamic_table_script` is `true`. Defaults to `{{table}}_DT`.
- `timeouts` (Block, Optional) (see [below for nested schema](#nestedblock--timeouts))
- `use_hybrid_tables` (Boolean) Specifies whether the connector should create Hybrid Tables. **Conditionally required** when `ingestion_mode` is `upsert`. Defaults to `false`.

### Read-Only

- `connector` (String) Connector type
- `id` (String) Unique identifier for the destination

<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

Optional:

- `create` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
- `delete` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
- `update` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
# Destination Snowflake can be imported by specifying the identifier.
terraform import streamkap_destination_snowflake.example-destination-snowflake 00000000000000000000000000
```
