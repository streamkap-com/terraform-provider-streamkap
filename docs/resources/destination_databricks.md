---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "streamkap_destination_databricks Resource - terraform-provider-streamkap"
subcategory: ""
description: |-
  Manages a Databricks destination connector.
  This resource creates and manages a Databricks destination for Streamkap data pipelines.
  Documentation https://docs.streamkap.com/streamkap-provider-for-terraform
---

# streamkap_destination_databricks (Resource)

Manages a **Databricks destination connector**.

This resource creates and manages a Databricks destination for Streamkap data pipelines.

[Documentation](https://docs.streamkap.com/streamkap-provider-for-terraform)



<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `name` (String) Name of the destination

### Optional

- `connection_timeout` (Number) Connection Timeout Defaults to `0`.
- `connection_url` (String) JDBC URL
- `consumer_wait_time_for_larger_batch_ms` (Number) The max wait time for larger batch size (in ms). The bigger the batch size, the the more cost effective loading will be on databricks but latency will grow as a trade-off. Defaults to `10000`.
- `databricks_catalog` (String) The name of the Databricks catalog to use. Defaults to `hive_metastore`.
- `databricks_token` (String, Sensitive) Token

**Security:** This value is marked sensitive and will not appear in CLI output or logs.
- `hard_delete` (Boolean) Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database Defaults to `false`.
- `ingestion_mode` (String) Upsert or append modes are available Defaults to `upsert`. Valid values: `upsert`, `append`.
- `partition_mode` (String) Partition tables or not Defaults to `by_topic`. Valid values: `by_topic`, `by_partition`, `by_topic_and_partition`.
- `schema_evolution` (String) Controls how schema evolution is handled by the sink connector. For pipelines with pre-created destination tables, set to `NONE` Defaults to `basic`. Valid values: `basic`, `none`.
- `table_name_prefix` (String) Schema for the associated table name Defaults to `streamkap`.
- `tasks_max` (Number) The maximum number of active tasks. NOTE: Increasing this value may increase parallelism and throughput but can also lead to higher costs on databricks side. Defaults to `5`.
- `timeouts` (Block, Optional) (see [below for nested schema](#nestedblock--timeouts))
- `topic2table_map` (Boolean) Falls back to Streamkap's default for tables where no match is found Defaults to `false`.
- `transforms_change_topic_name_mapping` (String) Map source tables to specific destination tables. Input should be the format of `source_table_name:destination_table_name` separated by a new line
- `transforms_change_topic_name_match_regex` (String) Regular expression for matching topic name parts to use as the destination table (database) or file (file storage) name

### Read-Only

- `connector` (String) Connector type
- `id` (String) Unique identifier for the destination

<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

Optional:

- `create` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
- `delete` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
- `update` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
# Destination Databricks can be imported by specifying the identifier.
terraform import streamkap_destination_databricks.example-destination-databricks 00000000000000000000000000
```
